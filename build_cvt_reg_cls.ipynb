{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.losses\n",
    "import tensorflow.keras.metrics\n",
    "import tensorflow.keras.optimizers\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Concatenate,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    GlobalAveragePooling1D,\n",
    "    Input,\n",
    "    LayerNormalization,\n",
    "    LeakyReLU,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from config import input_shape, input_shape_mf\n",
    "from cvt_custom_layers import LastStage, RearrangeLayer, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format(\"channels_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CVT_ortles_nu_flavour_evtau\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256  # Batch size\n",
    "num_classes = 2  # Number of classes. 1 for regression, 2 for binary classification, n for multiclass classification\n",
    "initial_dim = 16  #  Initial number of filters/channels in the network\n",
    "kernels = [(3, 9), (3, 6), 3]  # Square kernel dimensions for convolution layers\n",
    "strides = [(4, 4), (2, 4), 2]  # Stride values for convolutional layers\n",
    "heads = [1, 3, 6]  # Number of heads for the attention mechanism\n",
    "depth = [1, 2, 5]  # Refers to the number of layers\n",
    "dropout = 0.1  # Transformer dropout value\n",
    "scale_dim = 2  # Scaling factor for dim parameter over the 3 layers of cvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### X_h STAGE 1 ###\n",
    "dim = initial_dim\n",
    "target_h_input = Input(input_shape, name=\"target_h_in\")\n",
    "X_h = Conv2D(filters=dim, kernel_size=kernels[0], strides=strides[0], padding=\"same\")(\n",
    "    target_h_input\n",
    ")\n",
    "l, w = K.int_shape(X_h)[1], K.int_shape(X_h)[2]\n",
    "# X_h = CBAM()(X_h)\n",
    "X_h = RearrangeLayer(\n",
    "    dim=dim, length=math.ceil(l), width=math.ceil(w), compression=True\n",
    ")(X_h)\n",
    "X_h = LayerNormalization()(X_h)\n",
    "X_h = Transformer(\n",
    "    dim=dim,\n",
    "    length=math.ceil(l),\n",
    "    width=math.ceil(w),\n",
    "    depth=depth[0],\n",
    "    heads=heads[0],\n",
    "    dim_head=dim,\n",
    "    mlp_dim=dim * scale_dim,\n",
    "    dropout=dropout,\n",
    ")(X_h, training=True)\n",
    "X_h = RearrangeLayer(dim=dim, length=math.ceil(l), width=math.ceil(w))(X_h)\n",
    "\n",
    "### X_h STAGE 2 ###\n",
    "scale = heads[1] // heads[0]\n",
    "dim = scale * dim\n",
    "X_h = Conv2D(filters=dim, kernel_size=kernels[1], strides=strides[1], padding=\"same\")(\n",
    "    X_h\n",
    ")\n",
    "l, w = K.int_shape(X_h)[1], K.int_shape(X_h)[2]\n",
    "# X_h = CBAM()(X_h)\n",
    "X_h = RearrangeLayer(\n",
    "    dim=dim, length=math.ceil(l), width=math.ceil(w), compression=True\n",
    ")(X_h)\n",
    "X_h = LayerNormalization()(X_h)\n",
    "X_h = Transformer(\n",
    "    dim=dim,\n",
    "    length=math.ceil(l),\n",
    "    width=math.ceil(w),\n",
    "    depth=depth[1],\n",
    "    heads=heads[1],\n",
    "    dim_head=dim,\n",
    "    mlp_dim=dim * scale_dim,\n",
    "    dropout=dropout,\n",
    ")(X_h, training=True)\n",
    "X_h = RearrangeLayer(dim=dim, length=math.ceil(l), width=math.ceil(w))(X_h)\n",
    "\n",
    "### X_h STAGE 3 ###\n",
    "scale = heads[2] // heads[1]\n",
    "dim = scale * dim\n",
    "X_h = Conv2D(filters=dim, kernel_size=kernels[2], strides=strides[2], padding=\"same\")(\n",
    "    X_h\n",
    ")\n",
    "l, w = K.int_shape(X_h)[1], K.int_shape(X_h)[2]\n",
    "# X_h = CBAM()(X_h)\n",
    "X_h = RearrangeLayer(\n",
    "    dim=dim, length=math.ceil(l), width=math.ceil(w), compression=True\n",
    ")(X_h)\n",
    "X_h = LayerNormalization()(X_h)\n",
    "X_h = LastStage(dim=dim, batch_size=batch_size)(X_h, training=True)\n",
    "X_h = Transformer(\n",
    "    dim=dim,\n",
    "    length=math.ceil(l),\n",
    "    width=math.ceil(w),\n",
    "    depth=depth[2],\n",
    "    heads=heads[2],\n",
    "    dim_head=dim,\n",
    "    mlp_dim=dim * scale_dim,\n",
    "    dropout=dropout,\n",
    "    last_stage=True,\n",
    ")(X_h, training=True)\n",
    "if num_classes == 1:\n",
    "    X_h = GlobalAveragePooling1D()(X_h)\n",
    "else:\n",
    "    X_h = X_h[:, 0]\n",
    "\n",
    "X_h = LayerNormalization()(X_h)\n",
    "X_h = Dense(dim)(X_h)\n",
    "X_h = Flatten()(X_h)\n",
    "\n",
    "\n",
    "### X_v STAGE 1 ###\n",
    "dim = initial_dim\n",
    "target_v_input = Input(input_shape, name=\"target_v_in\")\n",
    "X_v = Conv2D(filters=dim, kernel_size=kernels[0], strides=strides[0], padding=\"same\")(\n",
    "    target_v_input\n",
    ")\n",
    "l, w = K.int_shape(X_v)[1], K.int_shape(X_v)[2]\n",
    "# X_v = CBAM()(X_v)\n",
    "X_v = RearrangeLayer(\n",
    "    dim=dim, length=math.ceil(l), width=math.ceil(w), compression=True\n",
    ")(X_v)\n",
    "X_v = LayerNormalization()(X_v)\n",
    "X_v = Transformer(\n",
    "    dim=dim,\n",
    "    length=math.ceil(l),\n",
    "    width=math.ceil(w),\n",
    "    depth=depth[0],\n",
    "    heads=heads[0],\n",
    "    dim_head=dim,\n",
    "    mlp_dim=dim * scale_dim,\n",
    "    dropout=dropout,\n",
    ")(X_v, training=True)\n",
    "X_v = RearrangeLayer(dim=dim, length=math.ceil(l), width=math.ceil(w))(X_v)\n",
    "\n",
    "### X_v STAGE 2 ###\n",
    "scale = heads[1] // heads[0]\n",
    "dim = scale * dim\n",
    "X_v = Conv2D(filters=dim, kernel_size=kernels[1], strides=strides[1], padding=\"same\")(\n",
    "    X_v\n",
    ")\n",
    "l, w = K.int_shape(X_v)[1], K.int_shape(X_v)[2]\n",
    "# X_v = CBAM()(X_v)\n",
    "X_v = RearrangeLayer(\n",
    "    dim=dim, length=math.ceil(l), width=math.ceil(w), compression=True\n",
    ")(X_v)\n",
    "X_v = LayerNormalization()(X_v)\n",
    "X_v = Transformer(\n",
    "    dim=dim,\n",
    "    length=math.ceil(l),\n",
    "    width=math.ceil(w),\n",
    "    depth=depth[1],\n",
    "    heads=heads[1],\n",
    "    dim_head=dim,\n",
    "    mlp_dim=dim * scale_dim,\n",
    "    dropout=dropout,\n",
    ")(X_v, training=True)\n",
    "X_v = RearrangeLayer(dim=dim, length=math.ceil(l), width=math.ceil(w))(X_v)\n",
    "\n",
    "### X_v STAGE 3 ###\n",
    "scale = heads[2] // heads[1]\n",
    "dim = scale * dim\n",
    "X_v = Conv2D(filters=dim, kernel_size=kernels[2], strides=strides[2], padding=\"same\")(\n",
    "    X_v\n",
    ")\n",
    "l, w = K.int_shape(X_v)[1], K.int_shape(X_v)[2]\n",
    "# X_v = CBAM()(X_v)\n",
    "X_v = RearrangeLayer(\n",
    "    dim=dim, length=math.ceil(l), width=math.ceil(w), compression=True\n",
    ")(X_v)\n",
    "X_v = LayerNormalization()(X_v)\n",
    "X_v = LastStage(dim=dim, batch_size=batch_size)(X_v, training=True)\n",
    "X_v = Transformer(\n",
    "    dim=dim,\n",
    "    length=math.ceil(l),\n",
    "    width=math.ceil(w),\n",
    "    depth=depth[2],\n",
    "    heads=heads[2],\n",
    "    dim_head=dim,\n",
    "    mlp_dim=dim * scale_dim,\n",
    "    dropout=dropout,\n",
    "    last_stage=True,\n",
    ")(X_v, training=True)\n",
    "if num_classes == 1:\n",
    "    X_v = GlobalAveragePooling1D()(X_v)\n",
    "else:\n",
    "    X_v = X_v[:, 0]\n",
    "\n",
    "X_v = LayerNormalization()(X_v)\n",
    "X_v = Dense(dim)(X_v)\n",
    "X_v = Flatten()(X_v)\n",
    "\n",
    "\n",
    "### X_mf_h STAGE 1 ###\n",
    "dim = initial_dim\n",
    "mufilter_in_h = Input(input_shape_mf, name=\"mufilter_in_h\")\n",
    "X_mf_h = Conv2D(\n",
    "    filters=dim, kernel_size=kernels[0], strides=strides[0], padding=\"same\"\n",
    ")(mufilter_in_h)\n",
    "l, w = K.int_shape(X_mf_h)[1], K.int_shape(X_mf_h)[2]\n",
    "# X_mf_h = CBAM()(X_mf_h)\n",
    "X_mf_h = RearrangeLayer(\n",
    "    dim=dim, length=math.ceil(l), width=math.ceil(w), compression=True\n",
    ")(X_mf_h)\n",
    "X_mf_h = LayerNormalization()(X_mf_h)\n",
    "X_mf_h = Transformer(\n",
    "    dim=dim,\n",
    "    length=math.ceil(l),\n",
    "    width=math.ceil(w),\n",
    "    depth=depth[0],\n",
    "    heads=heads[0],\n",
    "    dim_head=dim,\n",
    "    mlp_dim=dim * scale_dim,\n",
    "    dropout=dropout,\n",
    ")(X_mf_h, training=True)\n",
    "X_mf_h = RearrangeLayer(dim=dim, length=math.ceil(l), width=math.ceil(w))(X_mf_h)\n",
    "\n",
    "### X_mf_h STAGE 2 ###\n",
    "scale = heads[1] // heads[0]\n",
    "dim = scale * dim\n",
    "X_mf_h = Conv2D(\n",
    "    filters=dim, kernel_size=kernels[1], strides=strides[1], padding=\"same\"\n",
    ")(X_mf_h)\n",
    "l, w = K.int_shape(X_mf_h)[1], K.int_shape(X_mf_h)[2]\n",
    "# X_mf_h = CBAM()(X_mf_h)\n",
    "X_mf_h = RearrangeLayer(\n",
    "    dim=dim, length=math.ceil(l), width=math.ceil(w), compression=True\n",
    ")(X_mf_h)\n",
    "X_mf_h = LayerNormalization()(X_mf_h)\n",
    "X_mf_h = Transformer(\n",
    "    dim=dim,\n",
    "    length=math.ceil(l),\n",
    "    width=math.ceil(w),\n",
    "    depth=depth[1],\n",
    "    heads=heads[1],\n",
    "    dim_head=dim,\n",
    "    mlp_dim=dim * scale_dim,\n",
    "    dropout=dropout,\n",
    ")(X_mf_h, training=True)\n",
    "X_mf_h = RearrangeLayer(dim=dim, length=math.ceil(l), width=math.ceil(w))(X_mf_h)\n",
    "\n",
    "### X_mf_h STAGE 3 ###\n",
    "scale = heads[2] // heads[1]\n",
    "dim = scale * dim\n",
    "X_mf_h = Conv2D(\n",
    "    filters=dim, kernel_size=kernels[2], strides=strides[2], padding=\"same\"\n",
    ")(X_mf_h)\n",
    "l, w = K.int_shape(X_mf_h)[1], K.int_shape(X_mf_h)[2]\n",
    "# X_mf_h = CBAM()(X_mf_h)\n",
    "X_mf_h = RearrangeLayer(\n",
    "    dim=dim, length=math.ceil(l), width=math.ceil(w), compression=True\n",
    ")(X_mf_h)\n",
    "X_mf_h = LayerNormalization()(X_mf_h)\n",
    "X_mf_h = LastStage(dim=dim, batch_size=batch_size)(X_mf_h, training=True)\n",
    "X_mf_h = Transformer(\n",
    "    dim=dim,\n",
    "    length=math.ceil(l),\n",
    "    width=math.ceil(w),\n",
    "    depth=depth[2],\n",
    "    heads=heads[2],\n",
    "    dim_head=dim,\n",
    "    mlp_dim=dim * scale_dim,\n",
    "    dropout=dropout,\n",
    "    last_stage=True,\n",
    ")(X_mf_h, training=True)\n",
    "if num_classes == 1:\n",
    "    X_mf_h = GlobalAveragePooling1D()(X_mf_h)\n",
    "else:\n",
    "    X_mf_h = X_mf_h[:, 0]\n",
    "\n",
    "X_mf_h = LayerNormalization()(X_mf_h)\n",
    "X_mf_h = Dense(dim)(X_mf_h)\n",
    "X_mf_h = Flatten()(X_mf_h)\n",
    "\n",
    "\n",
    "### X_mf_v STAGE 1 ###\n",
    "dim = initial_dim\n",
    "mufilter_in_v = Input(input_shape_mf, name=\"mufilter_in_v\")\n",
    "X_mf_v = Conv2D(\n",
    "    filters=dim, kernel_size=kernels[0], strides=strides[0], padding=\"same\"\n",
    ")(mufilter_in_v)\n",
    "l, w = K.int_shape(X_mf_v)[1], K.int_shape(X_mf_v)[2]\n",
    "# X_mf_v = CBAM()(X_mf_v)\n",
    "X_mf_v = RearrangeLayer(\n",
    "    dim=dim, length=math.ceil(l), width=math.ceil(w), compression=True\n",
    ")(X_mf_v)\n",
    "X_mf_v = LayerNormalization()(X_mf_v)\n",
    "X_mf_v = Transformer(\n",
    "    dim=dim,\n",
    "    length=math.ceil(l),\n",
    "    width=math.ceil(w),\n",
    "    depth=depth[0],\n",
    "    heads=heads[0],\n",
    "    dim_head=dim,\n",
    "    mlp_dim=dim * scale_dim,\n",
    "    dropout=dropout,\n",
    ")(X_mf_v, training=True)\n",
    "X_mf_v = RearrangeLayer(dim=dim, length=math.ceil(l), width=math.ceil(w))(X_mf_v)\n",
    "\n",
    "### X_mf_v STAGE 2 ###\n",
    "scale = heads[1] // heads[0]\n",
    "dim = scale * dim\n",
    "X_mf_v = Conv2D(\n",
    "    filters=dim, kernel_size=kernels[1], strides=strides[1], padding=\"same\"\n",
    ")(X_mf_v)\n",
    "l, w = K.int_shape(X_mf_v)[1], K.int_shape(X_mf_v)[2]\n",
    "# X_mf_v = CBAM()(X_mf_v)\n",
    "X_mf_v = RearrangeLayer(\n",
    "    dim=dim, length=math.ceil(l), width=math.ceil(w), compression=True\n",
    ")(X_mf_v)\n",
    "X_mf_v = LayerNormalization()(X_mf_v)\n",
    "X_mf_v = Transformer(\n",
    "    dim=dim,\n",
    "    length=math.ceil(l),\n",
    "    width=math.ceil(w),\n",
    "    depth=depth[1],\n",
    "    heads=heads[1],\n",
    "    dim_head=dim,\n",
    "    mlp_dim=dim * scale_dim,\n",
    "    dropout=dropout,\n",
    ")(X_mf_v, training=True)\n",
    "X_mf_v = RearrangeLayer(dim=dim, length=math.ceil(l), width=math.ceil(w))(X_mf_v)\n",
    "\n",
    "### X_mf_v STAGE 3 ###\n",
    "scale = heads[2] // heads[1]\n",
    "dim = scale * dim\n",
    "X_mf_v = Conv2D(\n",
    "    filters=dim, kernel_size=kernels[2], strides=strides[2], padding=\"same\"\n",
    ")(X_mf_v)\n",
    "l, w = K.int_shape(X_mf_v)[1], K.int_shape(X_mf_v)[2]\n",
    "# X_mf_v = CBAM()(X_mf_v)\n",
    "X_mf_v = RearrangeLayer(\n",
    "    dim=dim, length=math.ceil(l), width=math.ceil(w), compression=True\n",
    ")(X_mf_v)\n",
    "X_mf_v = LayerNormalization()(X_mf_v)\n",
    "X_mf_v = LastStage(dim=dim, batch_size=batch_size)(X_mf_v, training=True)\n",
    "X_mf_v = Transformer(\n",
    "    dim=dim,\n",
    "    length=math.ceil(l),\n",
    "    width=math.ceil(w),\n",
    "    depth=depth[2],\n",
    "    heads=heads[2],\n",
    "    dim_head=dim,\n",
    "    mlp_dim=dim * scale_dim,\n",
    "    dropout=dropout,\n",
    "    last_stage=True,\n",
    ")(X_mf_v, training=True)\n",
    "if num_classes == 1:\n",
    "    X_mf_v = GlobalAveragePooling1D()(X_mf_v)\n",
    "else:\n",
    "    X_mf_v = X_mf_v[:, 0]\n",
    "\n",
    "X_mf_v = LayerNormalization()(X_mf_v)\n",
    "X_mf_v = Dense(dim)(X_mf_v)\n",
    "X_mf_v = Flatten()(X_mf_v)\n",
    "\n",
    "\n",
    "X = Concatenate()([X_h, X_v, X_mf_h, X_mf_v])\n",
    "X = Dense(128)(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = LeakyReLU(negative_slope=0.1)(X)\n",
    "X = Dropout(0.2)(X)\n",
    "X = Dense(64)(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = LeakyReLU(negative_slope=0.1)(X)\n",
    "X = Dropout(0.2)(X)\n",
    "\n",
    "if num_classes == 1:\n",
    "    X = Dense(1)(X)\n",
    "else:\n",
    "    activation = \"sigmoid\" if num_classes == 2 else \"softmax\"\n",
    "    units = 1 if num_classes == 2 else num_classes\n",
    "    X = Dense(units, activation=activation)(X)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[target_h_input, target_v_input, mufilter_in_h, mufilter_in_v],\n",
    "    outputs=X,\n",
    "    name=model_name,\n",
    ")\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "if num_classes == 1:\n",
    "    model.compile(\n",
    "        optimizer=\"Adam\",\n",
    "        loss=\"mse\",\n",
    "        metrics=[\n",
    "            \"mae\",\n",
    "        ],\n",
    "    )\n",
    "else:\n",
    "    if num_classes == 2:\n",
    "        model.compile(\n",
    "            optimizer=\"Adam\",\n",
    "            loss=tensorflow.keras.losses.BinaryFocalCrossentropy(from_logits=True),\n",
    "            metrics=[\n",
    "                tensorflow.keras.metrics.BinaryAccuracy(name=\"acc\", threshold=0.5),\n",
    "                tensorflow.keras.metrics.AUC(from_logits=True),\n",
    "            ],\n",
    "        )\n",
    "    else:\n",
    "        model.compile(\n",
    "            optimizer=\"Adam\",\n",
    "            loss=tensorflow.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[\n",
    "                tensorflow.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
    "                tensorflow.keras.metrics.AUC(from_logits=True),\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"model_zoo/{model_name}_b{batch_size}_e0.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "plot_model(\n",
    "    model,\n",
    "    to_file=(f\"model_zoo/{model_name}_b{batch_size}_e0.png\"),\n",
    "    show_shapes=True,\n",
    "    show_dtype=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",  # Top to bottom (TB) or left to right (LR)\n",
    "    expand_nested=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
